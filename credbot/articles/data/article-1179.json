{"id": 1179, "name": "Reporters Lab", "domain": "reporterslab.org", "bias": "center", "credibility": "high-credibility", "reporting": "very-high", "questionable": null, "url": "https://reporterslab.org/2024/06/20/researchers-mine-fact-check-insights-data-to-explore-many-facets-of-misinfo/", "title": "Researchers Mine Fact Check Insights Data To Explore Many Facets Of Misinfo", "content": "Fact-Checking News Fact-Checking Sitesopen dropdown menumap viewlist view map view list view map view list view About the Lab You Are Here:Home\u00bbResearchers mine Fact-Check Insights data to explore many facets of misinfo Fact-Checking News Structured Journalism **Researchers mine Fact-Check Insights data to explore many facets of misinfo** The dataset from the Duke Reporters\u2019 Lab has been downloaded hundreds of times By Erica Ryan - June 20, 2024 Researchers around the world are digging into the trove of data available from ourFact-Check Insightsproject, with plans to use it for everything from benchmarking the performance of large language models to studying the moderation of humor online. Since launching in December, the Fact-Check Insights dataset has been downloaded more than 300 times. The dataset is updated daily and made available at no cost to researchers by the Duke Reporters\u2019 Lab, with support from the Google News Initiative. Fact-Check Insights contains structured data from more than 240,000 claims made by political figures and social media accounts that have been analyzed and rated by independent fact-checkers. The dataset is powered by ClaimReview and MediaReview, twin tagging systems that allow fact-checkers to enter standardized data about their fact-checks, such as the statement being checked, the speaker, the date, and the rating. Users have found great value in the data. Marcin Sawi\u0144ski, a researcher in the Department of Information Systems at the Pozna\u0144 University of Economics and Business in Poland, is part of a team using ClaimReview data for a multiyear project aimed at developing a tool to assess the credibility of online sources and detect false information using AI. \u201cWith nearly a quarter of a million items reviewed by hundreds of fact-checking organizations worldwide, we gain instant access to a vast portion of the fact-checking output from the past several years,\u201d Sawi\u0144ski writes. \u201cManually tracking such a large number of fact-checking websites and performing web data extraction would be prohibitively labor-intensive. The ready-made dataset enables us to conduct comprehensive cross-lingual and cross-regional analyses of fake-news narratives with much less effort.\u201d The OpenFact project, which is financed by the National Center for Research and Development in Poland, uses natural language processing and machine learning techniques to focus on specific topics. \u201cShifting our efforts from direct web data extraction to the cleanup, disambiguation, and harmonization of ClaimReview data has significantly reduced our workload and increased our reach,\u201d Sawi\u0144ski writes. Other researchers who have downloaded the dataset plan to use it for benchmarking the performance of large language models for fact-checking uses. Others are investigating the response to false information by social media platforms. Ariadna Matamoros-Fern\u00e1ndez, a senior lecturer in digital media in the School of Communication at Queensland University of Technology in Australia, plans to use the Fact-Check Insights dataset as part of her research into identifying and moderating humor on digital platforms. \u201cI am using the dataset to find concrete examples of humorous posts that have been fact-checked to discuss these examples in interviews with factcheckers,\u201d Matamoros-Fern\u00e1ndez writes. \u201cI am also using the dataset to use examples of posts that have been flagged as being satire, memes, humour, parody etc to test whether different foundation models (GPT4/Gemini) are good at assessing these posts.\u201d The goals of her research include trying to \u201cbetter understand the dynamics of harmful humour online\u201d and creating best practices to tackle them. She has received a Discovery Early Career Researcher Award from the Australian Research Council to support her work. Rafael Aparecido Martins Frade, a doctoral student working with the Spanish fact-checking organization Newtral, plans to utilize the data in his research on using AI to tackle disinformation. \u201cI am currently researching automated fact-checking, namely multi-modal claim matching,\u201d he writes of his work. \u201cThe objective is to develop models and mechanisms to help fight the spread of fake news. Some of the applications we\u2019re planning to work on are euroscepticism, climate emergency and health.\u201d Researchers who have downloaded the Fact-Check Insights dataset have also provided the Reporters\u2019 Lab with feedback on making the data more usable. Enrico Zuccolotto, a master\u2019s degree student in artificial intelligence at the Polytechnic University of Milan, performed a thorough review of the dataset, offering suggestions aimed at reducing duplication and filling in missing data. While the data available from Fact-Check Insights is primarily presented in the original form submitted by fact-checking organizations, the Reporters\u2019 Lab has made small attempts to enhance the data\u2019s clarity, and we will continue to make such adjustments where feasible. Researchers who have questions about the dataset can refer to the \u201cGuide to the Data\u201d page, which includes a table outlining the fields included, along with examples (see the \u201cWhat you can expect when you download the data\u201d section). The Fact-Check Insights dataset is available for download in JSON and CSV formats. Access is free for researchers, journalists, technologists and others in the field, butregistrationis required. Related:What exactly is the Fact-Check Insights dataset? **Sidebar** The Reporters\u2019 Lab is a center for journalism research in the Sanford School of Public Policy at Duke University. Our core projects focus on fact-checking, but we also do occasional research about trust in the news media and other topics. Our projects include: A guide to the world\u2019s fact-checkers. We maintain a database of fact-checking sites and publish an annual census. Tech & Check, a collection of products and tools that use automation and other technologies to expand the reach of fact-checking and help fact-checkers do their work. Analysisabout trends and advances in fact-checking, including articles written by our students, staff and faculty. The Reporters\u2019 Lab is part of the DeWitt Wallace Center for Media & Democracy at the Sanford School."}