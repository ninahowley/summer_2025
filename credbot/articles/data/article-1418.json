{"id": 1418, "name": "The New Stack", "domain": "thenewstack.io", "bias": "center", "credibility": "high-credibility", "reporting": "high", "questionable": null, "url": "https://thenewstack.io/how-we-bootstrapped-eodhd-apis-from-a-10-server-to-a-global-data-infrastructure/", "title": "How We Bootstrapped Eodhd Apis From A 10 Server To A Global Data Infrastructure", "content": "We\u2019re so glad you\u2019re here. You can expect all the best TNS content to arrive Monday through Friday to keep you on top of the news and at the top of your game. Check your inbox for a confirmation email where you can adjust your preferences and even join additional groups. Follow TNS on your favorite social media networks. Become aTNS follower on LinkedIn. Check outthe latest featured and trending storieswhile you wait for your first TNS newsletter. **How We Bootstrapped EODHD APIs From a $10 Server to a Global Data Infrastructure** EODHD APIs offers a global stock market API providing fundamental and historical price data for stocks, ETFs, mutual funds, and bonds across major exchanges worldwide. We didn\u2019t start EODHD APIs with venture funding or a large team. In fact, it began with a single $10 server, a side hustle, and the desire to make high-quality financial data more accessible. No accelerators, no outside capital \u2014 just stubborn persistence and a lot of hands-on troubleshooting. We\u2019ve made countless mistakes along the way. We\u2019ve restructured entire systems, moved platforms, rewritten spaghetti code, and yes, migrated services in the middle of the night toavoid global downtime. This is the story of how a product person and an engineer bootstrapped a real-time data company and the lessons we learned in the process. **Phase 1: Unicorn Bay, Bootstrap Hustle, and Free Credits** In 2015, we were working full-time jobs and building what would eventually become EODHD APIs in our off-hours. The first iteration wasn\u2019t even EODHD APIs \u2014 it was Unicorn Bay, an investing platform built with PHP and Laravel on the backend, and Angular on the frontend. These were the tools we knew at the time \u2014 nothing fancy, just what worked. Since we had no budget, we hopped between providers offering free cloud credits: OVHcloud, other smaller players, and eventuallyAmazon AWS. We were granted $10,000 in AWS credits, which gave us breathing room and a fairly powerful 64 GB instance to host the service. But as traffic grew, we ran into a hidden cost: AWS charged heavily for outbound traffic. Our credits melted quickly. By April 2017, we knew we couldn\u2019t stay. We spun up a $10 frontend server on DigitalOcean and migrated everything. Global usage meant \u201cfollow the sun\u201d uptime \u2014 we had to plan downtime during the least painful time zones. Our first major migration happened with just a couple of hours of planned downtime and a lot of crossed fingers. **Phase 2: The Birth of EODHD APIs \u2014 One Weekend, One Client** The idea for EODHD APIs came to me on a Friday evening in 2017. By Monday, we had a working site, hooked up to PayPal, and one paying customer. From 2017 to 2019, I handled everything alone \u2014 backend, frontend, support, and documentation. It wasn\u2019t sustainable, but it worked. In November 2019,Evgenii Elesinrejoined the project full-time as a CTO. That\u2019s when we started paying real attention to architecture. But until then, our approach was always the same: build fast, fix later. Evgenii wanted clean services from the beginning. I preferred simple proxying to get things live. In the end, we compromised \u2014 and surprisingly, the \u201cquick and dirty\u201d approach scaled further than expected. We went from one shared server handling everything to five backend workers and separate servers for news, WebSocket streaming, and Clickhouse. **Phase 3: Scaling Painfully, Monitoring Everything** By 2020, things started breaking under load. Our architecture, never intended for heavy traffic, was hitting limits. We introduced real monitoring (Datadog) after a failed attempt to outsource observability. We rewrote legacy subsystems, isolated components, and slowly began migrating to modern stacks. We also discovered unexpected hotspots. Even our rate-limiting checks were bottlenecks due to inefficient MySQL queries. That\u2019s when Redis entered the picture \u2014 and helped drastically cut the latency. Today, we run on 17 servers. We have dedicated boxes for testing, development, different data pipelines, and public APIs. But the legacy remains. Some parts still run on old PHP and Laravel versions, and every modernization is a battle against time and priorities. **Infrastructure and Architecture** 1.1 Current Server Layout (17 total): We\u2019re primarily hosted on DigitalOcean droplets, with the following roles: Marketing site and API gateway MySQL server and data processing workers Five dedicated backend web worker servers News service (handles both data processing and web traffic) WebSocket server (Node.js) ClickHouse server Testing server Experimental servers: In-house ML (Python) In-house ML (Python) In-house ML (Python) External AI-based analytics External AI-based analytics External AI-based analytics Outsourced project support Outsourced project support Outsourced project support Supporting infrastructure server VPN server One bare-metal server used for CPU-intensive batch tasks 1.2 VM and Containerization Strategy: Core services run on virtual machines. Docker is used selectively \u2014 mainly for convenience in deployment andlocal dev environments, not for scaling. Supporting services often run as containers on the same VMs. 1.3 Dev Tooling in the Cloud: We use DigitalOcean\u2019s Docker registry and Bitbucket Pipelines to support development and deployment processes. **Real-Time Data and Queuing** 2.1 Queue Architecture: We use database-backed queues to distribute data processing workloads. Given the manageable frequency, volume, and complexity of updates, there\u2019s no need for complex message brokers like Kafka \u2014 a simpler in-DB queue is sufficient. 2.2 Real-Time Streaming: Our only true real-time stream is handled via a Node.js service built on Express and ws (WebSocket) modules. It proxies and streams time-sensitive market data. **Databases and Storage** 3.1 Database Stack: MySQL (InnoDB) remains the primary store for heavily interconnected core data. ClickHouse is used for high-volume, read-heavy datasets \u2014 mainly due to its excellent disk efficiency. Sphinx is integrated as a full-text search engine. Additional MySQL instances are used in support of various microservices. Redis is used for efficient rate-limiting and access control checks, significantly reducing latency compared to previous MySQL-based implementations. 3.2 Data Strategy: We lean on MySQL for transactional integrity and moderate update/delete activity. ClickHouse\u2019s append-only model isn\u2019t suited for all workloads, but it excels where analytics and performance matter. **CI/CD and Deployments** We don\u2019t need autoscaling or full-blown CI/CD due to the predictable nature of our workloads. However, we\u2019ve started adopting Bitbucket Pipelines forautomated checks and deployments. We\u2019re also exploring Laravel-native tools like Envoyer and Forge to further reduce human error and streamline releases. **Monitoring and Observability** We use: Sentry for error tracking and aggregated logs. Datadog for system metrics, internal/external service availability, and performance monitoring. This combination helps us investigate incidents and plan future scaling. **API Structure and Evolution** Early APIs often mirrored what clients expected from other providers, leading to inconsistencies. As the API market evolved and new data types were introduced, we began unifying endpoints and introducing versioning. Today: We use the JSON:API specification (with modifications for non-resource data). We document APIs using OpenAPI. We\u2019re still pragmatic \u2014 if clients needcustom formats to ease integration, we try to support them. **Stack and Technical Debt Management** 7.1 Current Tech Stack: We\u2019re running a fairly standard LEMP stack. Originally built on PHP 5.6, MySQL 5, and Laravel 5.7, we\u2019ve since upgraded most services to PHP 8.4, MySQL 8, and Laravel 12. However, some legacy services still run on PHP 7.4. 7.2 Maintenance Strategy: We now schedule regular \u201cfeature freeze\u201d or \u201caxe sharpening\u201d periods \u2014 1\u20132 weeks focused entirely onreducing technical debtand improving internal tools. This practice has delivered noticeable benefits. **Looking Ahead** EODHD is still growing. We\u2019re scaling responsibly now, investing in cleaner systems and a stronger team. But at its core, our story hasn\u2019t changed: build things people need, and fix them as you go. Just do it a little better each time. Community created roadmaps, articles, resources and journeys for developers to help you choose your path and grow in your career. Community created roadmaps, articles, resources and journeys for developers to help you choose your path and grow in your career."}